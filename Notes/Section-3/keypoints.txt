L - 11

Summary
This lecture covers various data preprocessing techniques essential for machine learning model implementation.

Highlights
💻 Importing libraries necessary for machine learning models.
📊 Importing datasets and handling missing data.
🔄 Encoding categorical data for preprocessing.
📚 Splitting datasets into training and test sets.
📏 Implementing feature scaling for better model performance.
🚀 Encouragement to take action and re-implement the tools provided.


L - 12

#   "import" That's just a command that will allow to import a library, or even a function or any type of modules.

#   A library is a symbol of modules containing functions and classes with which you can perform some actions and operations.

Summary
Learn how to import NumPy, Matplotlib, and Pandas libraries for machine learning.

Highlights
💻 NumPy is essential for working with arrays in machine learning models.
📊 Matplotlib allows for the plotting of charts and graphs.
📦 Pandas is useful for importing and pre-processing datasets.
🛠 Import libraries using the "import" command in Python.
📚 Libraries contain modules with functions and classes for performing actions.
🔄 Use shortcuts like NP for NumPy, PLT for Matplotlib, and PT for Pandas for faster access.
🧰 Build your toolkit with essential tools for data pre-processing and model building.


L - 13

#   In any data set with which you're gonna train a machine learning model, you have the same entities, which are the features and the dependent variable vector.

#   The features are the columns with which you are going to predict the dependent variable.

Summary
In this lesson, we learn how to import a dataset using Pandas read_csv() function. We also create the matrix of features and the dependent variable vector.

Highlights
💻 We import the dataset 'data.csv' using Pandas read_csv() function.
📊 The dataset consists of customer information like country, age, salary, and purchase status.
🧑‍🤝‍🧑 The features are the columns used to predict the dependent variable.
📈 The dependent variable is the column to be predicted.
📝 It is crucial to separate features and dependent variables in machine learning datasets.
🗂️ We create the matrix of features 'x' and the dependent variable vector 'y' from the dataset.
📑 The matrix of features contains the columns country, age, and salary while the dependent variable vector contains the purchase status column.


L - 14

#   ilock here stands for locate indexes and therefore what this function will do is it will take the indexes of the columns we want to extract from the data set. Not only the indexes of the columns but also the indexes of the rows.

#   We want to keep all the rows. And the trick to take all the rows, whatever data set you have, with whatever number of rows is to add here a colon.

#   Why is that? Because a column in Python means a range. And when we specify a range without the lower bound and neither the upper bound, that means in Python that we're taking everything in the range, therefore here's all the rows.

#   That means that we're taking the first index, the index zero, because indexes in Python start at zero. And then we're going up to minus one. So what does this minus one mean? Well, minus one means here, the last column. Minus one in Python means the index of the last column.

Summary
Using Pandas iloc for feature selection involves extracting the first three columns of a dataset to create the matrix of features X.

Highlights
💡 To create X, take the indexes of the first three columns of the dataset using iloc.
💡 Specify all rows and columns except the last one to create the matrix of features.
💡 Use a range of : -1 to automatically select all columns except the last one.
💡 The range in Python includes the lower bound but excludes the upper bound.
💡 Adding .values at the end retrieves all values in the selected rows and columns.
💡 This technique will be useful for creating future matrices of features in ML data preprocessing.


L - 15

Summary
In this lesson, the instructor explains how to create both the matrix of features (X) and the dependent variable vector (Y) for machine learning model training.

Highlights
💡 The dependent variable vector is typically the last column in the dataset.
💡 Using iloc to extract the desired rows and columns from the dataset.
💡 The index for selecting the last column is -1.
💡 X contains the independent variables such as country, age, and salary.
💡 Y contains the purchase decisions of customers.
💡 Creating X and Y separately is necessary for building machine learning models.
💡 The next step involves handling missing data in the dataset.


L - 17

#   First way is to just ignore the observation by deleting it. That's one method, and this actually works if you have a large data set, and you know, if you have only 1% missing data, you know removing 1% of the observations won't change much the learning quality of your model, so 1% is fine, but sometimes you can have a lot of missing data, and therefore you must handle them the right way.

#   So here we have a missing salary, what we want to do, is to replace this missing salary by the average of all these salaries. This is a classic way of handling missing data.

#   There are actually many replacements that you could do, you could, instead of replacing it by the average salary, you could replace it by the median salary

#   First, we have to specify which missing values we have to replace. And so that's why we have to enter here first argument called missing_values, which has to be equal to np, you know, the NumPy library, .nan, this is like an empty value, this is what this means, an empty value.

#   the second argument (exactly the one saying that indeed the missing_values here, the empty values of the data set will be replaced by the mean) And to do this, we have to add the next argument here, which is strategy, and this argument will be equal to, in quote, mean.

Summary
Using Scikit-Learn to replace missing values in machine learning involves handling missing data by either ignoring observations or replacing missing values with averages.

Highlights
💡 Ignoring missing data works if the dataset is large and only a small percentage of data is missing.
💡 Replacing missing values with the average of all values in the column is a classic way of handling missing data.
💡 Scikit-Learn, a powerful data science library, offers tools like SimpleImputer to handle missing data effectively.
💡 The SimpleImputer class from Scikit-Learn can be used to replace missing values with the mean of the feature itself.
💡 Different strategies like replacing missing values with the median or most frequent value are also options but using the mean is recommended.
💡 The process involves specifying the missing values to replace and the strategy to use, such as replacing with the mean.
💡 By creating an instance of SimpleImputer and specifying the arguments, missing values can be effectively replaced in the dataset.


L - 18

#   Remember that a class contains an assemble of instructions but also some operations and actions which you can apply to other objects or variables. And these are called methods. You know, they're like functions and one of them is exactly the fit method.

#   The fit method will exactly connect this imputer to the matrix of features. In other words, what this fit method will do is it will look at the missing values in, you know, the salary column and also it will compute the average of the salaries.

#   To do the replacement we'll have to call another method called transform and which will this time apply the transformation meaning it will replace the missing salary here by the average of the salaries.

#   It simply expects all the columns of X with numerical values but only the ones with numerical values not the ones with text or strings or categories.

#   So here we're specifying specific columns which are the age column and the salary column. And that's because we know that there is a missing salary. And by the way, there is also a missing age.

#   Final step we have to call the transform method once again from our imputer object. And so this transform method will exactly do that replacement of the missing salary here by the mean of the salaries. And same for the missing age. It'll be replaced by the mean of all the ages in the age column.

#   We of course have to input the columns of X where we want to replace missing data. And so these are the age column and the salary column. And therefore we simply have to input exactly the same as what was input in the fit method. So we're just gonna take this, copy this and paste that inside the transform method.

#   However, be careful, this transform method actually returns the new updated version of the matrix features X with the two replacements of the missing salary and the missing age. And therefore, what we want to do now and that's the last we have to do is to indeed update our matrix of features X.


./>     -----   exercise    -----
# Identify missing data (assumes that missing data is represented as NaN)
missing_data = df.isnull().sum()

# Print the number of missing entries in each column
print("Missing Data: \n", missing_data)
-----   end     -----

Summary

In this step, we use the SimpleImputer object to replace missing data in numerical columns of a dataset.

Highlights
💡 The fit method connects the imputer to the matrix of features and computes averages.
💡 The transform method replaces missing values with the averages.
💡 Specify only numerical columns to avoid errors when replacing missing data.
💡 Remember to include all numerical columns to ensure all missing data is replaced.
💡 Update the matrix of features by using the transform method to replace missing values.
💡 Check the updated matrix to verify that missing values have been replaced successfully.
💡 This step adds another valuable tool to your data processing toolkit.


L - 19

#   So we want to avoid the model to have such an interpretation because that could cause some misinterpreted correlations between the features and the outcome which we want to predict. Therefore, we can actually do much better than just encode these three countries into zero, one, and two.

#   We can do better is actually one hot encoding and one hot encoding consists of turning this country column into three columns.

#   One hot encoding consists of creating binary vectors for each of the countries. Let me explain this right away. So very simply, France would, for example have the vector 1 0 0. Spain would have the vector 0 1 0 and Germany would have the vector 0 0 1. So that then there is not a numerical order between the three countries because instead of having zero, one, and two, we would only have zeros and ones.

#   Remember that there is also this purchased columns that has labels, you know, non-numerical values with yes and nos. And we will actually have to replace them by zeros and ones. And that's totally fine for the dependent variable as long as it is a binary outcome, it is super fine. It will actually not compromise the future accuracy of the model if you just replace no and yes by zero and one.

#   To do this, we're going to use two classes. The first one is the column transformer class from the compose module of once again the scikit-learn library. And the second class is the one hot encoded class from the pre-processing module of the same Scikit-learn library.

./>
from sklearn.compose import ColumnTransform
from sklearn.preprocessing import OneHotEncoder
-----   end     -----

Summary
In this lesson, the instructor explains the importance of encoding categorical data for machine learning algorithms and introduces the concept of one-hot encoding.

Highlights
💡 Categorical data needs to be encoded into numerical values for machine learning models to compute correlations accurately.
💡 One-hot encoding involves creating binary vectors for each category in a column, eliminating numerical order and misinterpretations.
💡 This method is used to preprocess data sets with categorical variables, such as replacing non-numerical labels with zeros and ones.


L - 20

#   CT, which stands for, you know the object of the column transformer class. So CT equals, and now of course remember to create an instance or an object of a class you have to call the class itself. So column transformer class, there we go. Now since it is a class we have to add some parenthesis and site which we have to enter two arguments.

#   So these two arguments are first transformers, where we will specify, what kind of transformation we want to do and on which indexes of the columns we want to transform. And the second argument is remainder which will specify that we actually want to keep the columns that won't be applied some transformations meaning age and salary.

#   So for the first one, transformers we actually have to specify three things. First, the kind of transformation which is encoding. Second, what kind of encoding we want to do which is, one hot encoding. And third, the indexes of the columns we want to encode meaning country, the country column.

#   The second argument remainder. So here we want to specify in quote the following code name, which is passthrough, and which is a code name that will say that we indeed want to keep the columns that won't be applied some transformation that won't be one hot encoded Which are of course age and salary.

#   If we don't include this remainder equals passthrough here. Then when we apply the transformation on X we will only keep, you know the first three columns resulting from one hot encoding. And of course we want to keep age and salary into our matrix of features.

#   Our column transformer class actually has a method called fit transform which will do exactly the process of fitting and transforming at once at the same time.

#   Our CT object from which we're gonna call this, fit transform method, which will get as input well of course X because that's what we want to transform.

#   The fit transfer method actually doesn't return the output as a NumPy array. And it is absolutely compulsory to have X as NumPy array, because this will be expected by the future machinery models which we're gonna build. You know, in order to train the future machinery models where we're gonna use a train function, which is actually called fit. And this train function will expect the matrix of features X as a NumPy array. So here we want to force the output of this fit transfer method to be a NumPy array.

#   And to do this, we simply need to call well NumPy first which has a shortcut name NP from which we're gonna call this array function which will take as input exactly the output of the fit transform method.

./>
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[("encoder", OneHotEncoder, [0])] , remainder="passthrough")
X = np.array(ct.fit_transform(X))
-----   end     -----

Summary
In this step, we use ColumnTransformer to handle categorical data by implementing one-hot encoding.

Highlights
🔧 Create an object of the ColumnTransformer class named CT.
🔢 Specify the type of transformation, such as one-hot encoding, and the column indexes to transform.
🔄 Use the fit_transform method of CT to connect and transform the matrix of features X at once.
📊 Update the matrix of features X with the transformed data.
🧮 Convert the output of fit_transform to a NumPy array for compatibility with future machine learning models.


L - 21

#   The dependent variable because indeed it has a text format. No. And yes, and we would just like to convert these strings into zero and one respectively. And to do this, well that's very simple we're gonna use another class called "Label Encoder" and which will exactly encode these no's and yes's into zeros and ones respectively.

#   Then good news, we don't have to input anything in the parenthesis because you know we will just need to enter directly. Why? Because it is only one single vector. So it will be obvious what will be needed to encode.

#   Once again, there is a fit transform method which we can call directly on "Y" and which will exactly convert the no's and yes's, you know the text inside numerical values. And this time we don't have to have a non-binary because this is the dependent VO vector. It doesn't need to be a non-binary, you know as what is expected by the future missionary models. So we can just set the new "Y" to be what is returned by this fit transfer method, apply to the old Y with the texts no's and yes.

./>
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)
-----   end     -----

Summary
In this tutorial, the trainer demonstrates how to preprocess categorical data using one-hot encoding and label encoding techniques.
Highlights

💡 One-hot encoding transforms categorical data into numerical values using binary encoding.
💡 One-hot encoding creates new columns for each category, representing them as vectors of zeros and ones.
💡 Label encoding is used for binary outcomes, converting text labels into zeros and ones.
💡 Label encoding is simpler and directly encodes two classes into numerical values.
💡 Preprocessing categorical data is essential for preparing data for machine learning models.
💡 Understanding both one-hot encoding and label encoding expands the data preprocessing toolkit.
💡 The next step is to split the dataset into training and test sets for model training.


L - 22

#   One of the most frequently asked questions in the data science community; which is be ready for it. Do we have to apply feature scaling before splitting the data set into the training set and test set, or after?

#   So, the answer is we have to apply feature scaling after splitting the data set into the training set and the test set.

#   So, of course splitting the data set into the training set and a test set consists of making two separate sets. One training set where you're gonna train your machinery model on existing observations, and one test set where you're going to evaluate the performance of your model on new observations.

#   The new observations are exactly like, you know some future data that you're gonna get and on which you're going to deploy your machine learning model. Alright, so that's this first tool. And now feature scaling simply consists of scaling all your variables, all your features, actually, to make sure they all take values in the same scale.

#   Why? the test set is supposed to be a brand new set on which you're going to evaluate your machine learning model. So it's exactly like, you know, your training your machine learning model on your training set, and then later on, you know, after it is trained you're going to deploy it on new observations. So what this means is that the test set is something you're not supposed to work with for the training.

#   Feature scaling is, as you will see, a technique that will get the mean and the standard deviation of your feature, you know, in order to perform the scaling.

#   So, if we apply feature scaling before the split then it will actually get the mean and the standard deviation of all the values, including the ones in the test set. And since the test set is something you're not supposed to have, you know, like some future data in production, well you know, applying feature scaling on the original data set, before the split, would cause some what we call information leakage on the test set.

#   You know, we would grab some information from the test set, which we're not supposed to get, because it is supposed to be new data with new observations.

Summary
In this tutorial, the speaker explains the importance of splitting data into training and test sets and the timing of applying feature scaling in the process.

Highlights
💡 Splitting the data set into training and test sets is crucial for training and evaluating machine learning models.
💡 Feature scaling is necessary to ensure that all variables are in the same scale to prevent dominance of one feature over others.
💡 Feature scaling should be applied after splitting the data set to prevent information leakage on the test set.
💡 The test set should be treated as new data for model evaluation, and applying feature scaling before the split could leak information from the test set.
💡 Applying feature scaling after the split ensures that the test set remains untouched and maintains its integrity for model evaluation.


L - 23

#   We're basically going to get four sets, X train, which is the matrix of features of the training set. X test, which is the matrix of features of the test set Y train, which is the dependent variable of the training set, and Y test, which is the dependent variable of the test set. That's exactly what we want.

#   It expects, well, the combination of the matrix of features X, and the dependent variable vector Y. And that's the first two inputs of this function.

#   So, we still have to input two more arguments which are going to be first the split size, you know, because we're not going to split this data set into a training set and a test set of the same size. Actually, we need a lot of observations in the training set and a few in the test set, but we need a lot of them in the training set, so as to give the future machinery model more chance to understand and learn the correlations in the data set.

#   So, let me just tell you the recommended size of the split. Well, I recommend to have 80% observation in the training set, and 20% in the test set. All right, this is a very good split.

#   Because the observations will be randomly split into the training set and the test set. Well, to make sure we have the same random factors, we'll just set here, random_state = 1, right? We're just fixing the seed here so that we'll get the same split and therefore the same training set, and same test set.

Summary
In this tutorial, the instructor explains how to split a dataset into training and test sets using the train_test_split function from scikit-learn library in Python for machine learning models.

Highlights
💡 The train_test_split function creates four sets: X train, X test, Y train, and Y test.
✨ The format of these sets is crucial for the future machinery models that will be built.
📊 A recommended split size is 80% for training and 20% for testing.
🧩 The random_state parameter can be set for reproducibility of results.
🛠️ Utilizing Google Colab for efficient coding.
📝 Inputs for the train_test_split function include X (features) and Y (dependent variable) matrices.
🧪 The function allows for the creation of separate sets for training and testing machine learning models.


L - 24

Summary
In this lesson, the instructor demonstrates how to split a data set into training and test sets in Python, emphasizing the importance of avoiding information leakage by applying feature scaling after the split.

Highlights
💻 The code provided splits the data set into x train, y train, x test, and y test sets.
📊 x train contains features for the training set, including dummy variables, age, and salary.
🧪 x test contains features for the test set, with observations corresponding to random customers.
📈 y train and y test contain purchase decisions encoded with zeros and ones.
🛠️ Feature scaling should be applied after splitting the data to ensure the test set represents new observations.
🧩 This new tool in data preprocessing prepares for building future machine learning models.
🚀 The next step is to explore feature scaling and continue building predictive models.


*****   Coding Exercise 4   *****

L - 25

#   Well, that's because for some of the machinery models that's in order to avoid some features to be dominated by other features in such a way that the dominated features are not even considered by the machinery model.

#   For a lot of machinery models we won't even have to apply feature scaling even if we have features taking very different values.

#   Should we go for standardization or normalization? Well, we're gonna be here very pragmatic. Normalization is recommended when you have a normal distribution in most of your features. This will be a great feature scaling technique in that case. Standardization actually works well all the time. It will do the job all the time.

#   Therefore, since this is a technique that will work all the time and this is a technique that is more recommended for some specific situations where you have most of your features following a normal distribution then my ultimate recommendation for sure is to go for standardization. Because indeed this will always work, you will always do some relevant feature scaling and this will always improve the training process.

#   And I'm saying matrices of features because now we have two matrices of features which are X-train and X-test. And since we understood in the previous tutorial that feature scaling must be applied after the split while you understand that we won't apply feature scaling on the whole matrix of features X but of course on both X-train and X-test separately.

#   And actually the scaler will be fitted to only X-train and then we'll transform X-test. You know, we'll apply feature scaling on X-test because indeed since X-test is something that we're not supposed to have during the training,

Summary
Feature scaling is crucial in data preprocessing to put all features on the same scale, avoiding domination of certain features by others in machinery models. Two main techniques for feature scaling are standardization and normalization, with standardization being recommended for its consistent performance. Feature scaling should be applied separately to training and testing datasets to avoid data leakage.

Highlights
🔍 Feature scaling is essential to prevent domination of features in machinery models.
📏 Standardization and normalization are the main techniques for feature scaling.
📈 Standardization is recommended for consistent performance, especially when features follow a normal distribution.
🧮 Feature scaling should be applied separately to training and testing datasets.
🧩 Understanding the importance of fitting the scaler to the training set and transforming the test set.
💡 Clear explanation on when to apply feature scaling in data preprocessing.
🛠️ The implementation of feature scaling is crucial for improving the training process.


L - 26

#   Do we have to apply feature scaling you know, standardization to the dummy variables in the matrix of features? This is one of the most frequently asked questions.

#   The answer is no, The answer is no, because simply, well remember the goal of standardization or feature scaling in general, it is to have all the values of the features in the same range.

#   And since I told you that standardization actually transforms your features so that they take values between more or less minus three and plus three, while since here our dummy variables already take values between minus three and plus three because they're equal to either one or zero. Well, there is nothing extra to be done here with standardization.

#   And actually, standardization will only make it worse because indeed it will still transform these values between minus three and plus three. But then you will totally lose the interpretation of these variables. In other words, you will lose the information of which country corresponds to the observation.

#   Only apply feature scaling to your numerical values. Right here, we have clearly some variables taking values in a very different range, right? The age goes between zero and 100 and the salary goes between zero and 100,000. So clearly here, if it's better for the missionary model we have to apply feature scaling but let's leave these dummy variables alone so that we can keep the interpretability of the model.

Summary
Step 2 covers how to scale numeric features using the Standard Scaler class in scikit-learn for machine learning preprocessing.

Highlights
💡 Standard Scaler class in scikit-learn performs standardization on both training and test feature matrices.
💡 Feature scaling should not be applied to dummy variables as they are already within the same scale range.
💡 Applying feature scaling to dummy variables can lead to loss of interpretability and won't significantly improve model performance.
💡 Only apply feature scaling to numerical values that are in different ranges.
💡 Keeping dummy variables as they are helps maintain the interpretability of the model.
💡 Feature scaling is crucial for variables with different ranges like age and salary.
💡 Understanding when to apply feature scaling is key to effective data preprocessing.


L - 27

#   Which we called sc, from, which well, we're gonna use that fit method that will indeed, for each feature of X train, compute the mean of the feature, meaning the mean of the age, and then the mean of the salary. And then compute the standard deviation of the feature the age and the salary.

#   And that's exactly what the fit method will do. It will only compute the mean and the standard deviation of all the values.

#   And then you have the transform method that will indeed apply this formula by, you know, transforming each of the values here if each feature into this value resulting from this formula.

#   All right, so it's important to understand the difference between fit and transform. Fit will just get the mean and send the deviation of each of your features, and transform will apply this formula to indeed transform your values so that they can all be in the same scale.

#   the good news is that one of the methods of the standard scale of class is actually fit transform, which of course will proceed to the two tools at the same time. Meaning it'll fit your matrix of features to get the mean and standard deviation. And then right after that, transform all the values of the features to turn them into this formula.

#   And now obviously you know what to input inside this fit transform method. While that's of course exactly the same as extra in here because indeed we will only apply feature scaling to our numerical columns here containing non integer values, right? Non dummy variables, values.

Summary
The instructor explains how to fit and transform a scaler object on specific columns in a dataset for feature scaling.
Indexes in Python start from zero, and the instructor demonstrates how to specify the columns for feature scaling.
The fit method calculates the mean and standard deviation of each feature, while the transform method applies a formula to transform the values.
The fit_transform method combines fitting the scaler object and transforming the values in one step for efficiency.
Highlights
💡 The fit method computes the mean and standard deviation of features.
💡 Python indexes start from zero.
💡 The transform method applies a formula to transform feature values.
💡 The fit_transform method fits and transforms features in one step for efficiency.


L - 28

#   Now we have to also transform our matrix of features of the test set, meaning x_test this matrix of features. But since this data is like new data, which we get, you know, later on in production, well, for this data I will only apply the transform method because indeed the features of the test set need to be scaled by the same scaler that was used on the training set.

#   We cannot get a new scaler. You know, if we applied the fit transform method here on x_test, we would get a new scaler. And that would absolutely not make sense, because x_test will actually be the input of the predict function that will return the predictions, you know, after the machine learning model is trained.

#   And since this machine learning model will be trained with a particular scaler, you know, the scaler applied on the training set, well, in order to make predictions that will be congruent with the way the model was trained, well, we need to apply the same scaler that was used on the training set onto the test set, so that we can get indeed the same transformation, and therefore in the end, some relevant predictions with the predict method applied to x_test.

Summary
In this lesson, we learn about applying feature scaling to the test set using the same scaler as the training set in order to ensure consistency in predictions.

Highlights
💡 It is crucial to apply the same scaler used on the training set to the test set for consistent predictions.
💡 Using the transform method on the test set ensures that the features are scaled correctly.
💡 The transform method, not fit transform, should be used on the test set.
💡 Consistent scaling of variables helps improve the training of machine learning models.
💡 The data pre-processing template provided simplifies the process for future machine learning models.
💡 The template requires minimal changes, making it efficient for data pre-processing tasks.
💡 Understanding and implementing feature scaling is a key step before moving on to building machine learning models.

*****   Coding Exercise 5   *****
